# Звіт з Лабораторної роботи 1: Розпізнавання предметів одягу на Keras

### 1. Посилання на програмний код

Програмний код, використаний для виконання лабораторної роботи, доступний за посиланням на [Google Colab](https://colab.research.google.com/drive/187jYYfZFcxtzrk6fXpK-uPbHvM2Vt1x3#scrollTo=BNW4bWUYP8Rt):

---

### 2. Аналіз результатів базової версії програми

Базова версія нейронної мережі була навчена на наборі даних Fashion MNIST з наступними параметрами:

* **Архітектура:** Послідовна модель з одним прихованим повнов'язаним шаром (800 нейронів, активація "relu") та вихідним повнов'язаним шаром (10 нейронів, активація "softmax").
* **Оптимізатор:** SGD (Stochastic Gradient Descent).
* **Функція втрат:** categorical_crossentropy.
* **Кількість епох (`epochs`):** 100.
* **Розмір міні-вибірки (`batch_size`):** 200.
* **Розділення для валідації (`validation_split`):** 0.2 (20% тренувальних даних використовувались для валідації).

#### 2.1. Частка вірних відповідей на тестових даних

Після 100 епох навчання, базова модель продемонструвала наступну точність на тестовому наборі даних:

* **Частка вірних відповідей на тестових даних:** 86.61%

#### 2.2. Аналіз `val_accuracy` під час навчання

Протягом процесу навчання спостерігались наступні тенденції зміни валідаційної точності (`val_accuracy`):

* **Початковий швидкий ріст:** На перших епохах `val_accuracy` швидко зростала, демонструючи ефективне навчання моделі на ранніх стадіях. Наприклад, від 0.7366 на першій епосі до 0.8278 на десятій епосі.
* **Подальше зростання та стабілізація:** Зростання тривало, але поступово сповільнювалось.
* **Максимальне значення:** Найвище значення `val_accuracy` **0.8762** було досягнуто на **93-й епосі**.
* **Ознаки перенавчання (флуктуації/зниження):** Після 93-ї епохи `val_accuracy` почало флуктувати і дещо знижуватись (наприклад, до 0.8740 на 94-й епосі, 0.8720 на 100-й епосі). Це може свідчити про початок перенавчання моделі, коли вона починає "запам'ятовувати" шум у тренувальних даних замість узагальнення, що призводить до гіршої продуктивності на нових даних. Для цієї базової конфігурації оптимальна кількість епох для зупинки навчання була б близько 93.

#### 2.3. Розпізнавання зображень з набору Fashion MNIST

Для перевірки роботи навченої моделі, було обрано зображення з тестового набору даних (`n_rec = 10`).

* **Оригінальний об'єкт (правильна відповідь):** пальто
* **Розпізнано моделлю:** пальто
* **Оцінка:** Правильно

#### 2.4. Розпізнавання власного зображення

Було завантажено власне зображення білої футболки з файлу `Generated Image September 25, 2025 - 9_06AM.png`.

* **Оригінальний об'єкт:** Футболка
* **Розпізнано моделлю:** Туфлі
* **Оцінка:** Невірно. Модель не змогла коректно розпізнати власне зображення футболки, ідентифікувавши його як туфлі. Це може бути пов'язано з відмінностями у стилі зображення (фон, освітлення, ракурс) між Fashion MNIST та завантаженим файлом, або з обмеженою узагальнюючою здатністю базової моделі.

---

### 3. Експерименти з гіперпараметрами навчання

Проведено серію експериментів для оцінки впливу різних гіперпараметрів на якість навчання мережі. У кожному експерименті змінювався лише один гіперпараметр, інші залишалися базовими.

#### 3.1. Вплив кількості епох навчання (`epochs`)

| Кількість епох | Частка вірних відповідей на тестових даних (%) |
| :------------- | :-------------------------------------------: |
| 50             | 87.13                                         |
| 75             | 87.71                                         |
| 100            | 86.69                                         |
| 125            | **88.54**                                     |

* **Висновок:** Найвища точність (88.54%) була досягнута при **125 епохах**. Це вказує на те, що базова модель ще мала потенціал до навчання після 100 епох.

#### 3.2. Вплив розміру міні-вибірки (`batch_size`)

| Розмір міні-вибірки | Частка вірних відповідей на тестових даних (%) |
| :------------------ | :-------------------------------------------: |
| 50                  | **88.63**                                     |
| 100                 | 88.0                                          |
| 200                 | 86.69                                         |
| 400                 | 87.68                                         |

* **Висновок:** Найвища точність (88.63%) була досягнута при `batch_size = 50`. Менші розміри міні-вибірки часто призводять до більш точних оновлень градієнта, дозволяючи моделі краще досліджувати ландшафт втрат, хоча навчання може зайняти більше часу.

#### 3.3. Вплив кількості нейронів першого прихованого шару
  
| Кількість нейронів | Частка вірних відповідей на тестових даних (%) |
| :----------------- | :-------------------------------------------: |
| 500                | 86.45                                         |
| 700                | 86.63                                         |
| 800                | 86.69                                         |
| 900                | 86.51                                         |
| 1200               | **86.84**                                     |

* **Висновок:** Найкращий результат (86.84%) було отримано з **1200 нейронами**. Це свідчить про те, що для цього завдання більша кількість нейронів у першому прихованому шарі дозволяє моделі краще вивчити складні закономірності у вхідних даних.

#### 3.4. Вплив додавання прихованого шару

Було додано другий прихований шар (`Dense(XXX, activation="relu")`) між існуючим прихованим шаром (800 нейронів) та вихідним шаром.

| Кількість нейронів у другому прихованому шарі | Частка вірних відповідей на тестових даних (%) | Час навчання (орієнтовно) |
| :--------------------------------------------- | :-------------------------------------------: | :----------------------- |
| 500                                            | **87.67**                                     | 1m                       |
| 700                                            | 87.66                                         | 1m                       |
| 900                                            | 87.23                                         | 1m                       |
| 1200                                           | 87.42                                         | 1m                       |

* **Висновок:** Найкращий результат (87.67%) було досягнуто з **500 нейронами** у доданому прихованому шарі. Це значення, ймовірно, представляє оптимальний баланс між збільшенням складності моделі та ризиком перенавчання для цього конкретного набору даних та архітектури. Більші шари (700, 900, 1200 нейронів), хоча і мають більшу потенційну ємність, могли призвести до незначного перенавчання або ж до надмірної складності, яка не приносила суттєвих переваг на відносно простому наборі даних Fashion MNIST, де 800 нейронів першого шару вже виконують значну частину вилучення ознак. Час навчання при цьому **суттєво не збільшився** (залишився близько 1 хвилини в Google Colab), що є позитивним показником, оскільки дозволяє ускладнювати модель без значного зростання обчислювальних витрат.

---

### 4. Вибір найкращих гіперпараметрів та подальша оптимізація

На основі проведених експериментів були визначені найкращі значення гіперпараметрів:

* **Кількість епох:** 125
* **Розмір міні-вибірки:** 50
* **Кількість нейронів першого прихованого шару:** 1200
* **Кількість нейронів другого прихованого шару:** 500 (доданий шар)

Крім того, для подальшої оптимізації та запобігання перенавчанню, до архітектури були додані шари **Dropout** з коефіцієнтом 0.2 після кожного прихованого шару.

Була створена мережа з цією комбінацією архітектури та гіперпараметрів. Додатково було проведено експеримент зі зміною оптимізатора.

#### 4.1. Вплив зміни оптимізатора на оптимізовану модель

Була використана модель з найкращими гіперпараметрами (125 епох, batch_size=50, 1200 нейронів у першому шарі, 500 нейронів у другому прихованому шарі) та протестовані оптимізатори `Adam` та `SGD`.

| Оптимізатор | Частка вірних відповідей на тестових даних (%) |
| :---------- | :-------------------------------------------: |
| Adam        | 89.95                                         |
| SGD         | **90.03**                                     |

* **Висновок:** На оптимізованій архітектурі, оптимізатор **SGD показав трохи кращий результат (90.03%)**, ніж Adam (89.95%). Це цікавий результат, оскільки Adam часто дає кращі результати "з коробки", але для ретельно налаштованої мережі SGD може бути ефективнішим.

#### 4.2. Результати розпізнавання конкретного зображення оптимізованими моделями

Для того ж зображення, яке раніше було розпізнано неправильно, було проведено розпізнавання з використанням моделей, навчених з `Adam` та `SGD` (з найкращими гіперпараметрами).

* **Оптимізована модель з Adam:** Розпізнано як **Сумка**
* **Оптимізована модель з SGD:** Розпізнано як **Плаття**
* **Оцінка:** Обидві оптимізовані моделі все ще **неправильно** розпізнали зображення футболки. Це підкреслює складність узагальнення на зображення, що відрізняються від тренувального набору за стилем, освітленням та фоном.

#### 4.3. Чи збільшилася частка вірних відповідей нейромережі?

Так, частка вірних відповідей значно збільшилася. Базова модель мала точність 86.61%. Після оптимізації гіперпараметрів та вибору кращого оптимізатора, максимальна точність на тестових даних зросла до **90.03%**. Це є значним покращенням.

#### 4.4. Що можна зробити, щоб ще більше збільшити частку вірних відповідей?

Для подальшого збільшення частки вірних відповідей можна розглянути наступні кроки:

1. **Тюнінг шарів Dropout:** Як було зазначено в методичних вказівках, інтеграція шарів `Dropout` після прихованих шарів може допомогти запобігти перенавчанню та покращити узагальнення моделі. Це особливо актуально, якщо є ознаки перенавчання (як це спостерігалося з `val_accuracy` базової моделі).
2. **Використання інших оптимізаторів або їх конфігурацій:** Хоча `SGD` дав найкращий результат у цьому випадку, можна експериментувати з `SGD` з параметром `momentum` або більш глибоко налаштовувати `learning_rate` для `Adam`.
3. **Збільшення кількості прихованих шарів:** Експериментувати з додаванням ще більшої кількості прихованих шарів та їхньою конфігурацією нейронів.
4. **Аугментація даних (Data Augmentation):** Для покращення узагальнюючої здатності на нових зображеннях, можна використовувати аугментацію даних. Це включає створення нових тренувальних прикладів шляхом застосування випадкових перетворень до існуючих зображень (обертання, масштабування, зсуви, відображення).
